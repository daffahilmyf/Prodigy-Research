{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daffa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\daffa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text, options={}):\n",
    "    \n",
    "    exclude_words = [\"above\", \"after\", \"before\", \"between\", \"both\", \"but\", \"by\", \"during\", \"each\", \"for\", \"from\",\n",
    "                    \"further\", \"if\", \"in\", \"into\", \"more\", \"most\", \"not\", \"now\", \"off\", \"on\", \"once\", \"only\", \"or\",\n",
    "                    \"other\", \"out\", \"over\", \"so\", \"some\", \"such\", \"than\", \"that\", \"then\", \"there\", \"these\", \"under\",\n",
    "                    \"untill\", \"when\", \"where\", \"which\", \"while\", \"will\", \"with\", \"because\"]\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove Contraction\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove Punction\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if options.get(\"remove_stopwords\", True):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Exclude specific conditional words\n",
    "        if options.get(\"exclude_conditional_stopwords\", True):\n",
    "            stop_words = set([word for word in stop_words if word not in exclude_words])\n",
    "        \n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize text\n",
    "    if options.get(\"lemmatize\", True):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Stem text\n",
    "    if options.get(\"stem\", True):\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into text string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recount_tokens(preproccesing_text):\n",
    "    tokens = []\n",
    "    words = preproccesing_text.split()\n",
    "    start = 0\n",
    "    for i, word in enumerate(words):\n",
    "        # Check if word has leading whitespace\n",
    "        ws = True\n",
    "        # Add token to list\n",
    "        tokens.append({\n",
    "            'text': word,\n",
    "            'start': start,\n",
    "            'end': start+len(word),\n",
    "            'id': i,\n",
    "            'ws': ws\n",
    "        })\n",
    "        # Move start index to next word\n",
    "        start += len(word) + 1\n",
    "        \n",
    "    # Set the ws of the last token to False\n",
    "    if tokens:\n",
    "        tokens[-1]['ws'] = False\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_index(text, string_index):\n",
    "    tokens = text.split()\n",
    "    token_start_index = 0\n",
    "    for token in tokens:\n",
    "        token_end_index = token_start_index + len(token)\n",
    "        if string_index >= token_start_index and string_index <= token_end_index:\n",
    "            return tokens.index(token)\n",
    "        token_start_index = token_end_index + 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recount_spans(dataset, preprocess_text, options):\n",
    "    new_spans = []\n",
    "    \n",
    "    if 'spans' in dataset:\n",
    "    \n",
    "        for span in dataset['spans']:\n",
    "            \n",
    "            start = span['start']\n",
    "            end = span['end']\n",
    "            text_pre = text_preprocess(dataset['text'][start:end], options)\n",
    "            \n",
    "            new_start = preprocess_text.find(text_pre)\n",
    "            new_end = new_start + len(text_pre)\n",
    "            \n",
    "        \n",
    "            new_token_start = get_token_index(preprocess_text, new_start)\n",
    "            new_token_end = get_token_index(preprocess_text, new_end)\n",
    "            \n",
    "            new_spans.append({\n",
    "                'start': new_start,\n",
    "                'end': new_end,\n",
    "                'token_start':  new_token_start,\n",
    "                'token_end': new_token_end,\n",
    "                'label': span['label']\n",
    "            })\n",
    "            \n",
    "        return new_spans\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preprocessing(input_datasets, options):\n",
    "    pre_datasets = input_datasets\n",
    "    for dataset in tqdm(pre_datasets):\n",
    "        preprocessing_text = text_preprocess(dataset['text'], options=options)\n",
    "        new_spans = recount_spans(dataset, preprocessing_text, options=options)\n",
    "        new_tokens = recount_tokens(preprocessing_text)\n",
    "        \n",
    "        dataset['text'] = preprocessing_text\n",
    "        dataset['spans'] = new_spans\n",
    "        dataset['tokens'] = new_tokens\n",
    "        \n",
    "    return pre_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "def save_to_jsonlines(objects, filename):\n",
    "    with jsonlines.open(filename, mode='w') as writer:\n",
    "        for obj in objects:\n",
    "            writer.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "options = {\n",
    "    'remove_stopwords': True, \n",
    "    'lemmatize': True, \n",
    "    'stem': True,\n",
    "    'exclude_conditional_stopwords': True\n",
    "}\n",
    "\n",
    "option_values = itertools.product([True, False], repeat=len(options))\n",
    "\n",
    "\n",
    "for values in option_values:\n",
    "    # Create a new options dictionary with the current combination of values\n",
    "    current_options = {key: value for key, value in zip(options.keys(), values)}\n",
    "    \n",
    "    dataset = []\n",
    "    # Open the file in read mode\n",
    "    with open('./assets/annotated/annotations.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Load the JSON data from the file\n",
    "            dataset.append(json.loads(line.strip()))\n",
    "\n",
    "    dataset_preprocess = dataset_preprocessing(dataset, current_options)\n",
    "    \n",
    "    # Create filename based on current options\n",
    "    filename = \"combination\"\n",
    "    for key, value in current_options.items():\n",
    "        filename += f\"-{str(value)[0]}\"\n",
    "    filename += \".jsonl\"\n",
    "    \n",
    "    filepath = f\"./assets/preprocess/{filename}\"\n",
    "    save_to_jsonlines(dataset_preprocess, f\"{filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\daffa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\daffa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 316/316 [00:01<00:00, 257.21it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import spacy\n",
    "import jsonlines\n",
    "import json\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def text_preprocess(text, options={}):\n",
    "    \n",
    "    exclude_words = [\"above\", \"after\", \"before\", \"between\", \"both\", \"but\", \"by\", \"during\", \"each\", \"for\", \"from\",\n",
    "                    \"further\", \"if\", \"in\", \"into\", \"more\", \"most\", \"not\", \"now\", \"off\", \"on\", \"once\", \"only\", \"or\",\n",
    "                    \"other\", \"out\", \"over\", \"so\", \"some\", \"such\", \"than\", \"that\", \"then\", \"there\", \"these\", \"under\",\n",
    "                    \"untill\", \"when\", \"where\", \"which\", \"while\", \"will\", \"with\", \"because\"]\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove Contraction\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove Punction\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if options.get(\"remove_stopwords\", True):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # Exclude specific conditional words\n",
    "        if options.get(\"exclude_conditional_stopwords\", True):\n",
    "            stop_words = set([word for word in stop_words if word not in exclude_words])\n",
    "        \n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize text\n",
    "    if options.get(\"lemmatize\", True):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Stem text\n",
    "    if options.get(\"stem\", True):\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into text string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def recount_tokens(preproccesing_text):\n",
    "    tokens = []\n",
    "    words = preproccesing_text.split()\n",
    "    start = 0\n",
    "    for i, word in enumerate(words):\n",
    "        # Check if word has leading whitespace\n",
    "        ws = True\n",
    "        # Add token to list\n",
    "        tokens.append({\n",
    "            'text': word,\n",
    "            'start': start,\n",
    "            'end': start+len(word),\n",
    "            'id': i,\n",
    "            'ws': ws\n",
    "        })\n",
    "        # Move start index to next word\n",
    "        start += len(word) + 1\n",
    "        \n",
    "    # Set the ws of the last token to False\n",
    "    if tokens:\n",
    "        tokens[-1]['ws'] = False\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def get_token_index(text, string_index):\n",
    "    tokens = text.split()\n",
    "    token_start_index = 0\n",
    "    for token in tokens:\n",
    "        token_end_index = token_start_index + len(token)\n",
    "        if string_index >= token_start_index and string_index <= token_end_index:\n",
    "            return tokens.index(token)\n",
    "        token_start_index = token_end_index + 1\n",
    "    return -1\n",
    "\n",
    "def recount_spans(dataset, preprocess_text, options):\n",
    "    new_spans = []\n",
    "    \n",
    "    if 'spans' in dataset:\n",
    "    \n",
    "        for span in dataset['spans']:\n",
    "            \n",
    "            start = span['start']\n",
    "            end = span['end']\n",
    "            text_pre = text_preprocess(dataset['text'][start:end], options)\n",
    "            \n",
    "            new_start = preprocess_text.find(text_pre)\n",
    "            new_end = new_start + len(text_pre)\n",
    "            \n",
    "        \n",
    "            new_token_start = get_token_index(preprocess_text, new_start)\n",
    "            new_token_end = get_token_index(preprocess_text, new_end)\n",
    "            \n",
    "            new_spans.append({\n",
    "                'start': new_start,\n",
    "                'end': new_end,\n",
    "                'token_start':  new_token_start,\n",
    "                'token_end': new_token_end,\n",
    "                'label': span['label']\n",
    "            })\n",
    "            \n",
    "        return new_spans\n",
    "    return []\n",
    "\n",
    "def dataset_preprocessing(input_datasets, options):\n",
    "    pre_datasets = input_datasets\n",
    "    for dataset in tqdm(pre_datasets):\n",
    "        preprocessing_text = text_preprocess(dataset['text'], options=options)\n",
    "        new_spans = recount_spans(dataset, preprocessing_text, options=options)\n",
    "        new_tokens = recount_tokens(preprocessing_text)\n",
    "        \n",
    "        dataset['text'] = preprocessing_text\n",
    "        dataset['spans'] = new_spans\n",
    "        dataset['tokens'] = new_tokens\n",
    "        \n",
    "    return pre_datasets\n",
    "\n",
    "\n",
    "\n",
    "def save_to_jsonlines(objects, filename):\n",
    "    with jsonlines.open(filename, mode='w') as writer:\n",
    "        for obj in objects:\n",
    "            writer.write(obj)\n",
    "\n",
    "options = {\n",
    "    'remove_stopwords': True, \n",
    "    'lemmatize': True, \n",
    "    'stem': True,\n",
    "    'exclude_conditional_stopwords': True\n",
    "}\n",
    "\n",
    "dataset = []\n",
    "    # Open the file in read mode\n",
    "with open('./assets/annotated/annotations.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        # Load the JSON data from the file\n",
    "        dataset.append(json.loads(line.strip()))\n",
    "\n",
    "dataset_preprocess = dataset_preprocessing(dataset, options)\n",
    "\n",
    "filename= \"test\"\n",
    "filepath = f\"./assets/preprocess/{filename}\"\n",
    "save_to_jsonlines(dataset_preprocess, f\"{filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
